# ğŸ CTF RL Project

A multi-agent reinforcement learning (MARL) project simulating a Capture-the-Flag (CTF) environment using advanced RL algorithms like DQN, PPO, MAPPO, and MADDPG. The project provides a competitive environment for evaluating coordination, communication, and strategic learning among agents.

## ğŸ“ Project Structure

```
ctf_rl_project/
â”œâ”€â”€ pycache/ # Compiled bytecode files
â”œâ”€â”€ logs/ # Training logs and model checkpoints
â”œâ”€â”€ anacopy.py # Model cloning or architecture utility
â”œâ”€â”€ analyze_results.py # Evaluation and result plotting
â”œâ”€â”€ buffer.py # Replay buffer implementation
â”œâ”€â”€ commnet.py # Agent communication neural network
â”œâ”€â”€ ctf_env.py # Custom CTF environment
â”œâ”€â”€ dqn.py # DQN algorithm
â”œâ”€â”€ logger.py # Logging utility
â”œâ”€â”€ maddpg.py # MADDPG algorithm
â”œâ”€â”€ mappo.py # MAPPO algorithm
â”œâ”€â”€ play.py # Run trained agents
â”œâ”€â”€ ppo.py # PPO algorithm
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ tarmac.py # Map and agent placement logic
â”œâ”€â”€ train_visual.py # Visualized training
â”œâ”€â”€ train.py # Main training script
â”œâ”€â”€ visualization.py # Training visualization tools
â””â”€â”€ wrappers.py # Gym wrappers
```

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/ctf_rl_project.git
cd ctf_rl_project
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

**Note:** Python 3.8+ is recommended. You may need additional packages such as gym, pygame, numpy, and matplotlib.

## ğŸ§  Algorithms Implemented

- Deep Q-Network (DQN)
- Proximal Policy Optimization (PPO)
- Multi-Agent PPO (MAPPO)
- Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
- Communication Networks (CommNet)

## ğŸ—ï¸ Environment Description

A custom 15Ã—15 grid world simulating a competitive Capture-the-Flag game between two teams:
- Red Team vs. Blue Team
- Objective: Capture opponent's flag & defend own
- Obstacles and terrains add strategy
- Partial observability for realistic scenarios
- Action Space: {Stay, Up, Down, Left, Right} per agent
- Observation Space: Local 5Ã—5 grid patches per agent

## ğŸ‹ï¸â€â™‚ï¸ Training Agents

### Basic Training
```bash
python train.py --algo ppo --env ctf
```

### Visualized Training
```bash
python train_visual.py --algo maddpg
```

## ğŸ§ª Evaluation

### Run Trained Agent
```bash
python play.py --model checkpoints/ppo_final.pth --env ctf
```

### Analyze Results
```bash
python analyze_results.py
```

## ğŸ“Š Visualization

Generate reward plots, success graphs, and other metrics:
```bash
python visualization.py --logdir logs/mappo/
```

## ğŸ” Logging

Training logs include:
- Episode reward history
- Win/loss outcomes
- Loss trends
- Model checkpoints

All saved in the `logs/` directory and structured via `logger.py`.

## ğŸ§‘â€ğŸ’» Contributors

- Bandaru Ritesh Kumar (CS22B2043)
- Ganesh Banotu (CS22B2008)

## ğŸ“„ License

This project is released under the MIT License.
(Or replace with your own license.)

## ğŸ™ Acknowledgements

This project was developed as part of CS3009 - Reinforcement Learning at IIITDM Kancheepuram, under the guidance of Dr. Rahul Raman.

## ğŸŒ References

- MADDPG Paper (Lowe et al., 2017)
- CommNet Paper (Sukhbaatar et al., 2016)
- MAPPO Paper (Yu et al., 2022)
